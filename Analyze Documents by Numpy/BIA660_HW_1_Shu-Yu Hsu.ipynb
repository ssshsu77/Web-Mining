{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>HW #1: Analyze Documents by Numpy</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: \n",
    "- Please read the problem description carefully\n",
    "- Make sure to complete all requirements (shown as bullets) . In general, it would be much easier if you complete the requirements in the order as shown in the problem description\n",
    "- Follow the Submission Instruction to submit your assignment.\n",
    "- Code of academic integrity:\n",
    "    - **Each assignment needs to be completed independently. This is NOT group assignment**. \n",
    "    - Never ever copy others' work (even with minor modification, e.g. changing variable names)\n",
    "    - If you generate code using large lanaguage models (although it is not encouraged), make sure to adapt the generated code to meet all requirements and it is executable.\n",
    "    - Anti-Plagiarism software will be used to check similarities between all submissions.\n",
    "    - Check Syllabus for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Description**\n",
    "\n",
    "In this assignment, you'll write functions to analyze an article to find out the word distributions and key concepts. \n",
    "\n",
    "The packages you'll need for this assignment include `numpy` and `string`. Some useful functions:\n",
    "- string, list, dictionary: `split`,`join`, `count`, `index`,`strip`\n",
    "- numpy: `sum`, `where`,`log`, `argsort`,`argmin`, `argmax` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Define a function to analyze word counts in a document\n",
    "\n",
    "\n",
    "Define a function named `tokenize(doc)` which process an input document (denoted as `doc`) as follows: \n",
    "\n",
    "* First convert the document to lower case.\n",
    "* Split the document into a list of tokens by **space** (including tabs and new lines). For example, `Hello, it's a helloooo world!` -> `[\"Hello,\", \"it's\", \"a\", \"helloooo\", \"world!\"]` \n",
    "* Remove leading or trailing punctuations of each token. For example, `world!` ->`world`, but `it's` is not changed as the punctiation is in the middle. \n",
    "    - Hint, you can import module *string*, use `string.punctuation` to get a list of punctuations (say `puncts`), and then use function `strip(puncts)` to remove leading or trailing punctuations in each token\n",
    "* Find the count of each unique `non-empty` token and save the count as a dictionary, named `vocab`, i.e., `{\"Hello,\": 1, a: 1, ...}` \n",
    "* Return the dictionary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pprint as pp\n",
    "# add your input statement\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 1, \"it's\": 1, 'a': 1, 'helloooo': 1, 'world': 1}"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(doc):\n",
    "    \n",
    "    vocab = {}\n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    doc=doc.lower() # Convert the doc to lower case\n",
    "    tokens=doc.split() # Split the doc into a list of tokens by space\n",
    "    tokens=[token.strip(string.punctuation) for token in tokens if token.strip(string.punctuation)] \n",
    "    # Remove leading or trailing punctuations of each token and filter out empty strings\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            vocab[token]+=1\n",
    "        else:\n",
    "            vocab[token]=1 \n",
    "    return vocab\n",
    "\n",
    "#test\n",
    "doc = \"Hello , it's a helloooo world!\"\n",
    "vocab = tokenize(doc)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 1, \"it's\": 1, 'a': 1, 'helloooo': 1, 'world': 1}"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function\n",
    "\n",
    "doc = \"Hello , it's a helloooo world!\"\n",
    "vocab = tokenize(doc)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Split unusual words into common pieces \n",
    "\n",
    "\n",
    "Notice that some words contains extra characters or punctuations. Next we'll find the common subwords in each word (e.g., split \"helloooo\" to \"hello\" and \"ooo\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1.** Define a function `get_pair_count(vocab)` to count the freqency of two subwords in a word as follows:\n",
    "\n",
    "\n",
    "- The input is a dictionary (denoted as `vocab`) which maps each word into its count. The word contains subwords delimited by space. For example, at the beginning, we treat each character as a subword. Thus, the `vocab` from Q1 is `{\"h e l l o\":1, \"a\":1, ...}`\n",
    "- Count any pair of consecutive subwords in each word and create a new dictionary to note down the total count of each pair across all the words, e.g. `{\"e l\": 2}`.\n",
    "- Return the dictionary for the subword pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'h e l l o': 1, 'h e l l o o o o': 1, \"i t ' s\": 1, 'w o r l d': 1}\n",
      "\n",
      "\n",
      "{(\"'\", 's'): 1,\n",
      " ('e', 'l'): 2,\n",
      " ('h', 'e'): 2,\n",
      " ('i', 't'): 1,\n",
      " ('l', 'd'): 1,\n",
      " ('l', 'l'): 2,\n",
      " ('l', 'o'): 2,\n",
      " ('o', 'o'): 3,\n",
      " ('o', 'r'): 1,\n",
      " ('r', 'l'): 1,\n",
      " ('t', \"'\"): 1,\n",
      " ('w', 'o'): 1}\n"
     ]
    }
   ],
   "source": [
    "def get_pair_count(vocab):\n",
    "\n",
    "    pairs = {}\n",
    "\n",
    "    # add your code here\n",
    "    for word, count in vocab.items():\n",
    "        subwords = word.split() # Split the word into characters\n",
    "        for i in range(len(subwords) - 1):\n",
    "            pair = (subwords[i], subwords[i + 1]) # Consider each adjacent pair of characters\n",
    "            pairs[pair] = pairs.get(pair, 0) + count # Update the count of the pair\n",
    "    return pairs\n",
    "\n",
    "# Test\n",
    "\n",
    "# At the start, treat each character as a subword. \n",
    "# Add spaces as delimiters of subwords in each word \n",
    "\n",
    "init_vocab = {' '.join(list(word)) : count for word, count in vocab.items()}\n",
    "pp.pprint(init_vocab)\n",
    "print(\"\\n\")\n",
    "\n",
    "pairs = get_pair_count(init_vocab)\n",
    "pp.pprint(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'h e l l o': 1, 'h e l l o o o o': 1, \"i t ' s\": 1, 'w o r l d': 1}\n",
      "\n",
      "\n",
      "{(\"'\", 's'): 1,\n",
      " ('e', 'l'): 2,\n",
      " ('h', 'e'): 2,\n",
      " ('i', 't'): 1,\n",
      " ('l', 'd'): 1,\n",
      " ('l', 'l'): 2,\n",
      " ('l', 'o'): 2,\n",
      " ('o', 'o'): 3,\n",
      " ('o', 'r'): 1,\n",
      " ('r', 'l'): 1,\n",
      " ('t', \"'\"): 1,\n",
      " ('w', 'o'): 1}\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "# At the start, treat each character as a subword. \n",
    "# Add spaces as delimiters of subwords in each word \n",
    "\n",
    "init_vocab = {' '.join(list(word)) : count for word, count in vocab.items()}\n",
    "pp.pprint(init_vocab)\n",
    "print(\"\\n\")\n",
    "\n",
    "pairs = get_pair_count(init_vocab)\n",
    "pp.pprint(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2**. Define a function `merge_subwords(pair, vocab)` as follows:\n",
    "\n",
    "\n",
    "- The inputs include a subword pair (denoted as `pair`), and the original vocabulary dictionary (denoted as `vocab`).\n",
    "- For each word in `vocab`, if it contains `pair`, remove the space delimiter between the pair. Now this pair becomes a new subword. \n",
    "    - Hint: if you know regular expression, feel free to use it here. Otherwise, you can simply use function `replace`. Don't worry about some minor cross-boundary issues, e.g., `('hell' 'o')` may be matched with `hell oo`.\n",
    "- Return the new vocabuary dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'he l l o': 1, 'he l l o o o o': 1, \"i t ' s\": 1, 'w o r l d': 1}\n"
     ]
    }
   ],
   "source": [
    "def merge_subwords(pair, vocab):\n",
    "\n",
    "    # initialize output vocab\n",
    "    vcab_out = {}\n",
    "\n",
    "    # add your code here\n",
    "    for word, count in vocab.items():\n",
    "        # replace subword pair separated by space with merged subword\n",
    "        new_subwords = word.replace(pair[0] + ' ' + pair[1], pair[0] + pair[1])\n",
    "        vcab_out[new_subwords] = count\n",
    "    return vcab_out\n",
    "\n",
    "# Test\n",
    "\n",
    "pair = ('h', 'e')\n",
    "\n",
    "# replace all 'h e' substrings by 'he'\n",
    "new_vocab = merge_subwords(pair, init_vocab)\n",
    "\n",
    "pp.pprint(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'he l l o': 1, 'he l l o o o o': 1, \"i t ' s\": 1, 'w o r l d': 1}\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "pair = ('h', 'e')\n",
    "\n",
    "# replace all 'h e' substrings by 'he'\n",
    "new_vocab = merge_subwords(pair, init_vocab)\n",
    "\n",
    "pp.pprint(new_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3**. Define a function `subword_tokenize(doc, num_merges = 5)` to put all functions together.\n",
    "\n",
    "\n",
    "- The inputs include a document (denoted as `doc`) and the number of times to merge subwords.\n",
    "- Call `tokenize(doc)` to get the initial vocabulary dictionary, denoted as `vocab`\n",
    "- For each word in `vocab`, add a space delimiter between characters to indict that each character is treated as a subword initially. Save these charaters into a list named `subwords`\n",
    "- Repeat the follow steps for `num_merges` times:\n",
    "    - Call `get_pair_count(vocab)` to get the frequency of subword pair across the words\n",
    "    - Find the subword pair with the highest count, denoted as `pair`. If there is a tie, take any pair.\n",
    "    - Call `merge_subwords(pair, vocab)` to merge the selected subwords and update the vocabulary `vocab`. Add the new subword into the list `subwords`.\n",
    "- Finally, split each word in `vocab` by space to generate a new dictionary for the count of each subword.\n",
    "- Return the subword dictionary and also `subwords` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge #0:\n",
      "pairs:  {('o', 'o'): 3, ('h', 'e'): 2, ('e', 'l'): 2, ('l', 'l'): 2, ('l', 'o'): 2, ('w', 'o'): 2, ('o', 'r'): 2, ('r', 'l'): 2, ('l', 'd'): 2, ('i', 't'): 1, ('t', \"'\"): 1, (\"'\", 's'): 1}\n",
      "pair:  ('o', 'o')\n",
      "Vocab: {'h e l l o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'h e l l oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo']\n",
      "\n",
      "\n",
      "Merge #1:\n",
      "pairs:  {('h', 'e'): 2, ('e', 'l'): 2, ('l', 'l'): 2, ('w', 'o'): 2, ('o', 'r'): 2, ('r', 'l'): 2, ('l', 'd'): 2, ('l', 'o'): 1, ('i', 't'): 1, ('t', \"'\"): 1, (\"'\", 's'): 1, ('l', 'oo'): 1, ('oo', 'oo'): 1}\n",
      "pair:  ('h', 'e')\n",
      "Vocab: {'he l l o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'he l l oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he']\n",
      "\n",
      "\n",
      "Merge #2:\n",
      "pairs:  {('he', 'l'): 2, ('l', 'l'): 2, ('w', 'o'): 2, ('o', 'r'): 2, ('r', 'l'): 2, ('l', 'd'): 2, ('l', 'o'): 1, ('i', 't'): 1, ('t', \"'\"): 1, (\"'\", 's'): 1, ('l', 'oo'): 1, ('oo', 'oo'): 1}\n",
      "pair:  ('he', 'l')\n",
      "Vocab: {'hel l o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'hel l oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel']\n",
      "\n",
      "\n",
      "Merge #3:\n",
      "pairs:  {('hel', 'l'): 2, ('w', 'o'): 2, ('o', 'r'): 2, ('r', 'l'): 2, ('l', 'd'): 2, ('l', 'o'): 1, ('i', 't'): 1, ('t', \"'\"): 1, (\"'\", 's'): 1, ('l', 'oo'): 1, ('oo', 'oo'): 1}\n",
      "pair:  ('hel', 'l')\n",
      "Vocab: {'hell o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'hell oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell']\n",
      "\n",
      "\n",
      "Merge #4:\n",
      "pairs:  {('w', 'o'): 2, ('o', 'r'): 2, ('r', 'l'): 2, ('l', 'd'): 2, ('hell', 'o'): 1, ('i', 't'): 1, ('t', \"'\"): 1, (\"'\", 's'): 1, ('hell', 'oo'): 1, ('oo', 'oo'): 1}\n",
      "pair:  ('w', 'o')\n",
      "Vocab: {'hell o': 1, 'wo r l d': 2, \"i t ' s\": 1, 'a': 1, 'hell oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'wo']\n",
      "\n",
      "\n",
      "Merge #5:\n",
      "pairs:  {('wo', 'r'): 2, ('r', 'l'): 2, ('l', 'd'): 2, ('hell', 'o'): 1, ('i', 't'): 1, ('t', \"'\"): 1, (\"'\", 's'): 1, ('hell', 'oo'): 1, ('oo', 'oo'): 1}\n",
      "pair:  ('wo', 'r')\n",
      "Vocab: {'hell o': 1, 'wor l d': 2, \"i t ' s\": 1, 'a': 1, 'hell oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'wo', 'wor']\n",
      "\n",
      "\n",
      "Merge #6:\n",
      "pairs:  {('wor', 'l'): 2, ('l', 'd'): 2, ('hell', 'o'): 1, ('i', 't'): 1, ('t', \"'\"): 1, (\"'\", 's'): 1, ('hell', 'oo'): 1, ('oo', 'oo'): 1}\n",
      "pair:  ('wor', 'l')\n",
      "Vocab: {'hell o': 1, 'worl d': 2, \"i t ' s\": 1, 'a': 1, 'hell oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'wo', 'wor', 'worl']\n",
      "\n",
      "\n",
      "Merge #7:\n",
      "pairs:  {('worl', 'd'): 2, ('hell', 'o'): 1, ('i', 't'): 1, ('t', \"'\"): 1, (\"'\", 's'): 1, ('hell', 'oo'): 1, ('oo', 'oo'): 1}\n",
      "pair:  ('worl', 'd')\n",
      "Vocab: {'hell o': 1, 'world': 2, \"i t ' s\": 1, 'a': 1, 'hell oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'wo', 'wor', 'worl', 'world']\n",
      "\n",
      "\n",
      "Merge #8:\n",
      "pairs:  {('hell', 'o'): 1, ('i', 't'): 1, ('t', \"'\"): 1, (\"'\", 's'): 1, ('hell', 'oo'): 1, ('oo', 'oo'): 1}\n",
      "pair:  ('hell', 'o')\n",
      "Vocab: {'hello': 1, 'world': 2, \"i t ' s\": 1, 'a': 1, 'helloo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'wo', 'wor', 'worl', 'world', 'hello']\n",
      "\n",
      "\n",
      "vocab:\n",
      "{'a': 1, 'hello': 1, 'helloo oo': 1, \"i t ' s\": 1, 'world': 2}\n",
      "subwords:\n",
      "['h',\n",
      " 'e',\n",
      " 'l',\n",
      " 'o',\n",
      " 'w',\n",
      " 'r',\n",
      " 'd',\n",
      " 'i',\n",
      " 't',\n",
      " \"'\",\n",
      " 's',\n",
      " 'a',\n",
      " 'oo',\n",
      " 'he',\n",
      " 'hel',\n",
      " 'hell',\n",
      " 'wo',\n",
      " 'wor',\n",
      " 'worl',\n",
      " 'world',\n",
      " 'hello']\n"
     ]
    }
   ],
   "source": [
    "def subword_tokenize(doc, num_merges = 5):\n",
    "    vocab_out = {}\n",
    "    subwords = []\n",
    "    # add your code here\n",
    "    vocab_out = tokenize(doc)\n",
    "    \n",
    "    for word, count in vocab_out.items():\n",
    "        characters = list(word)\n",
    "        for char in characters:\n",
    "            if char not in subwords:\n",
    "                subwords.append(char)\n",
    "\n",
    "    vocab_out = {\" \".join(list(word)) : count for word, count in vocab_out.items()} \n",
    "\n",
    "    for i in range(num_merges):\n",
    "        print(\"Merge #\"+str(i)+\":\")\n",
    "        pairs = get_pair_count(vocab_out)\n",
    "        pair = max(pairs, key=pairs.get)\n",
    "        print(\"pairs: \", dict(sorted(pairs.items(), key=operator.itemgetter(1), reverse=True)))\n",
    "        print(\"pair: \",pair)\n",
    "\n",
    "        merged_subword = pair[0] + pair[1]\n",
    "        vocab_out = merge_subwords(pair, vocab_out) \n",
    "        subwords.append(merged_subword) \n",
    "        print(\"Vocab:\", vocab_out)\n",
    "        print(\"Subwords:\", subwords)\n",
    "\n",
    "        print(\"\\n\")\n",
    "    \n",
    "\n",
    "    return vocab_out, subwords\n",
    "\n",
    "# test\n",
    "# for debugging, you can print out the result of each merge as shown below.\n",
    "\n",
    "doc = \"Hello world, it's a helloooo world!\"\n",
    "\n",
    "vocab_out, subwords = subword_tokenize(doc, num_merges = 9)\n",
    "\n",
    "print(\"vocab:\")\n",
    "pp.pprint(vocab_out)\n",
    "\n",
    "print(\"subwords:\")\n",
    "pp.pprint(subwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge #0: \n",
      "pair: ('o', 'o') \n",
      "Vocab:\t{'h e l l o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'h e l l oo oo': 1} \n",
      "Subwords:\t['e', 'r', 'a', 'i', 's', 'h', 'l', \"'\", 't', 'd', 'o', 'w', 'oo'] \n",
      "\n",
      "Merge #1: \n",
      "pair: ('h', 'e') \n",
      "Vocab:\t{'he l l o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'he l l oo oo': 1} \n",
      "Subwords:\t['e', 'r', 'a', 'i', 's', 'h', 'l', \"'\", 't', 'd', 'o', 'w', 'oo', 'he'] \n",
      "\n",
      "Merge #2: \n",
      "pair: ('he', 'l') \n",
      "Vocab:\t{'hel l o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'hel l oo oo': 1} \n",
      "Subwords:\t['e', 'r', 'a', 'i', 's', 'h', 'l', \"'\", 't', 'd', 'o', 'w', 'oo', 'he', 'hel'] \n",
      "\n",
      "Merge #3: \n",
      "pair: ('hel', 'l') \n",
      "Vocab:\t{'hell o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'hell oo oo': 1} \n",
      "Subwords:\t['e', 'r', 'a', 'i', 's', 'h', 'l', \"'\", 't', 'd', 'o', 'w', 'oo', 'he', 'hel', 'hell'] \n",
      "\n",
      "Merge #4: \n",
      "pair: ('w', 'o') \n",
      "Vocab:\t{'hell o': 1, 'wo r l d': 2, \"i t ' s\": 1, 'a': 1, 'hell oo oo': 1} \n",
      "Subwords:\t['e', 'r', 'a', 'i', 's', 'h', 'l', \"'\", 't', 'd', 'o', 'w', 'oo', 'he', 'hel', 'hell', 'wo'] \n",
      "\n",
      "Merge #5: \n",
      "pair: ('wo', 'r') \n",
      "Vocab:\t{'hell o': 1, 'wor l d': 2, \"i t ' s\": 1, 'a': 1, 'hell oo oo': 1} \n",
      "Subwords:\t['e', 'r', 'a', 'i', 's', 'h', 'l', \"'\", 't', 'd', 'o', 'w', 'oo', 'he', 'hel', 'hell', 'wo', 'wor'] \n",
      "\n",
      "Merge #6: \n",
      "pair: ('wor', 'l') \n",
      "Vocab:\t{'hell o': 1, 'worl d': 2, \"i t ' s\": 1, 'a': 1, 'hell oo oo': 1} \n",
      "Subwords:\t['e', 'r', 'a', 'i', 's', 'h', 'l', \"'\", 't', 'd', 'o', 'w', 'oo', 'he', 'hel', 'hell', 'wo', 'wor', 'worl'] \n",
      "\n",
      "Merge #7: \n",
      "pair: ('worl', 'd') \n",
      "Vocab:\t{'hell o': 1, 'world': 2, \"i t ' s\": 1, 'a': 1, 'hell oo oo': 1} \n",
      "Subwords:\t['e', 'r', 'a', 'i', 's', 'h', 'l', \"'\", 't', 'd', 'o', 'w', 'oo', 'he', 'hel', 'hell', 'wo', 'wor', 'worl', 'world'] \n",
      "\n",
      "Merge #8: \n",
      "pair: ('hell', 'o') \n",
      "Vocab:\t{'hello': 1, 'world': 2, \"i t ' s\": 1, 'a': 1, 'hell oo oo': 1} \n",
      "Subwords:\t['e', 'r', 'a', 'i', 's', 'h', 'l', \"'\", 't', 'd', 'o', 'w', 'oo', 'he', 'hel', 'hell', 'wo', 'wor', 'worl', 'world', 'hello'] \n",
      "\n",
      "vocab:\n",
      "defaultdict(<class 'int'>,\n",
      "            {   \"'\": 1,\n",
      "                'a': 1,\n",
      "                'hell': 1,\n",
      "                'hello': 1,\n",
      "                'i': 1,\n",
      "                'oo': 2,\n",
      "                's': 1,\n",
      "                't': 1,\n",
      "                'world': 2})\n",
      "subwords:\n",
      "[   'e',\n",
      "    'r',\n",
      "    'a',\n",
      "    'i',\n",
      "    's',\n",
      "    'h',\n",
      "    'l',\n",
      "    \"'\",\n",
      "    't',\n",
      "    'd',\n",
      "    'o',\n",
      "    'w',\n",
      "    'oo',\n",
      "    'he',\n",
      "    'hel',\n",
      "    'hell',\n",
      "    'wo',\n",
      "    'wor',\n",
      "    'worl',\n",
      "    'world',\n",
      "    'hello']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# for debugging, you can print out the result of each merge as shown below.\n",
    "\n",
    "doc = \"Hello world, it's a helloooo world!\"\n",
    "\n",
    "vocab_out, subwords = subword_tokenize(doc, num_merges = 9)\n",
    "\n",
    "print(\"vocab:\")\n",
    "pp.pprint(vocab_out)\n",
    "\n",
    "print(\"subwords:\")\n",
    "pp.pprint(subwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Generate a document term matrix (DTM) as a numpy array\n",
    "\n",
    "\n",
    "Define a function `get_dtm(docs)` as follows:\n",
    "- The input is a list of documents, denoted as `docs`\n",
    "- For each document, call `tokenize(doc)` defined in **Q1** (let's only use the simple version for now) to get the vocabulary dictionary \n",
    "- Pool the keys from all the dictionaries to get a list of unique words, denoted as `unique_words` \n",
    "- Creates a numpy array (denoted as `dtm`) with a shape of (# of documents x # of unique words), and set the initial values to 0. \n",
    "- Fill cell `dtm[i,j]` with the count of the `j`th word in the `i`th document \n",
    "- Return `dtm` and `unique_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test document collection). This document can be found at https://hbr.org/2022/04/the-power-of-natural-language-processing\n",
    "\n",
    "# treat each paragraph as a document\n",
    "\n",
    "docs = open(\"chatgpt.txt\", 'r').readlines()\n",
    "\n",
    "dtm, words = get_dtm(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 314)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"Ethan Mollick has a message for the humans and the machines: can't we all just get along?\\n\""
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['ethan',\n",
       " 'mollick',\n",
       " 'has',\n",
       " 'a',\n",
       " 'message',\n",
       " 'for',\n",
       " 'the',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'machines',\n",
       " \"can't\",\n",
       " 'we',\n",
       " 'all',\n",
       " 'just',\n",
       " 'get',\n",
       " 'along']"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['22-year-old',\n",
      " 'a',\n",
      " 'a.i',\n",
      " 'about',\n",
      " 'abroad',\n",
      " 'academic',\n",
      " 'access',\n",
      " 'acknowledge',\n",
      " 'adapt',\n",
      " 'admits',\n",
      " 'adopted',\n",
      " 'after',\n",
      " 'again',\n",
      " 'against',\n",
      " 'agrees',\n",
      " 'all',\n",
      " 'allowing',\n",
      " 'almost',\n",
      " 'along',\n",
      " 'already',\n",
      " 'alternates',\n",
      " 'an',\n",
      " 'and',\n",
      " 'anxiety',\n",
      " 'any',\n",
      " 'app',\n",
      " 'are',\n",
      " 'artificial',\n",
      " 'as',\n",
      " 'asked',\n",
      " 'asking',\n",
      " 'assessments',\n",
      " 'associate',\n",
      " 'at',\n",
      " 'away',\n",
      " 'b',\n",
      " 'b-minus',\n",
      " 'banned',\n",
      " 'be',\n",
      " 'been',\n",
      " 'before',\n",
      " 'behalf',\n",
      " 'believes',\n",
      " 'between',\n",
      " 'bot',\n",
      " \"bot's\",\n",
      " 'but',\n",
      " 'by',\n",
      " 'calculators',\n",
      " 'can',\n",
      " \"can't\",\n",
      " 'capability',\n",
      " 'challenge',\n",
      " 'change',\n",
      " 'changed',\n",
      " 'changes',\n",
      " 'chatbot',\n",
      " 'chatgpt',\n",
      " 'cheating',\n",
      " 'check',\n",
      " 'cites',\n",
      " 'class',\n",
      " 'classes',\n",
      " 'classroom',\n",
      " 'code',\n",
      " 'come',\n",
      " 'company',\n",
      " 'compose',\n",
      " 'computer',\n",
      " 'concerns',\n",
      " 'convinced',\n",
      " 'core',\n",
      " 'could',\n",
      " \"couldn't\",\n",
      " 'course',\n",
      " 'crashed',\n",
      " 'created',\n",
      " 'deserve',\n",
      " 'despite',\n",
      " 'detect',\n",
      " 'did',\n",
      " \"didn't\",\n",
      " 'differently',\n",
      " 'districts',\n",
      " 'do',\n",
      " \"don't\",\n",
      " 'earlier',\n",
      " 'early',\n",
      " 'educators',\n",
      " 'edward',\n",
      " 'emerging',\n",
      " 'enthusiasm',\n",
      " 'entrepreneurship',\n",
      " 'errors',\n",
      " 'essays',\n",
      " 'estimates',\n",
      " 'ethan',\n",
      " 'even',\n",
      " 'ever',\n",
      " 'everybody',\n",
      " 'everyone',\n",
      " 'exam',\n",
      " 'exams',\n",
      " 'experimenting',\n",
      " 'facilitate',\n",
      " 'failure',\n",
      " 'far',\n",
      " 'fed',\n",
      " 'final',\n",
      " 'first',\n",
      " 'for',\n",
      " 'formally',\n",
      " 'found',\n",
      " 'from',\n",
      " 'further',\n",
      " 'generate',\n",
      " 'get',\n",
      " 'given',\n",
      " 'going',\n",
      " 'good',\n",
      " 'gptzero',\n",
      " 'grander',\n",
      " 'great',\n",
      " 'guardrails',\n",
      " 'had',\n",
      " 'happening',\n",
      " 'has',\n",
      " 'have',\n",
      " 'he',\n",
      " 'his',\n",
      " 'honest',\n",
      " 'honesty',\n",
      " 'how',\n",
      " 'human',\n",
      " 'humans',\n",
      " 'i',\n",
      " \"i'm\",\n",
      " 'ideas',\n",
      " 'if',\n",
      " 'importantly',\n",
      " 'in',\n",
      " 'incorrect',\n",
      " 'indications',\n",
      " 'information',\n",
      " 'innovation',\n",
      " 'intelligence',\n",
      " 'interactions',\n",
      " 'interrogated',\n",
      " 'into',\n",
      " 'is',\n",
      " \"isn't\",\n",
      " 'it',\n",
      " \"it's\",\n",
      " 'its',\n",
      " 'just',\n",
      " 'kenya',\n",
      " 'know',\n",
      " 'launched',\n",
      " 'like',\n",
      " 'lot',\n",
      " 'machine',\n",
      " 'machines',\n",
      " 'many',\n",
      " 'math',\n",
      " 'may',\n",
      " 'maybe',\n",
      " 'mba',\n",
      " 'me',\n",
      " 'mean',\n",
      " 'message',\n",
      " 'misleading',\n",
      " 'mollick',\n",
      " \"mollick's\",\n",
      " 'month',\n",
      " 'most',\n",
      " 'motivation',\n",
      " 'move',\n",
      " 'named',\n",
      " 'nature',\n",
      " 'need',\n",
      " 'new',\n",
      " 'not',\n",
      " 'november',\n",
      " 'now',\n",
      " 'npr',\n",
      " 'occasionally',\n",
      " 'of',\n",
      " 'officially',\n",
      " 'omissions',\n",
      " 'on',\n",
      " 'one',\n",
      " 'only',\n",
      " 'openai',\n",
      " 'or',\n",
      " 'other',\n",
      " 'overuse',\n",
      " 'partially',\n",
      " 'pass',\n",
      " \"pennsylvania's\",\n",
      " 'people',\n",
      " 'perhaps',\n",
      " 'place',\n",
      " 'places',\n",
      " 'poetry',\n",
      " 'points',\n",
      " 'policies',\n",
      " 'policy',\n",
      " 'popular',\n",
      " 'post-chatgpt',\n",
      " 'prestigious',\n",
      " 'princeton',\n",
      " 'probably',\n",
      " 'problems',\n",
      " 'professor',\n",
      " 'project',\n",
      " 'projects',\n",
      " 'prompts',\n",
      " 'provided',\n",
      " 'put',\n",
      " 'questions',\n",
      " 'raised',\n",
      " 'ran',\n",
      " 'readily',\n",
      " 'reads',\n",
      " 'reason',\n",
      " 'reasons',\n",
      " 'recently',\n",
      " 'require',\n",
      " 'required',\n",
      " 'responsible',\n",
      " 'result',\n",
      " 'results',\n",
      " 'right',\n",
      " 'running',\n",
      " 'said',\n",
      " 'scale',\n",
      " 'school',\n",
      " 'session',\n",
      " 'set',\n",
      " 'share',\n",
      " 'should',\n",
      " 'shying',\n",
      " 'since',\n",
      " 'skill',\n",
      " 'so',\n",
      " 'solve',\n",
      " 'some',\n",
      " 'something',\n",
      " 'sources',\n",
      " 'stanford',\n",
      " 'states',\n",
      " 'stop',\n",
      " 'stopped',\n",
      " 'student',\n",
      " 'students',\n",
      " 'stuff',\n",
      " 'sudden',\n",
      " 'surprising',\n",
      " 'survey',\n",
      " 'syllabus',\n",
      " 'taught',\n",
      " 'teach',\n",
      " 'teaches',\n",
      " 'tell',\n",
      " 'testing',\n",
      " 'that',\n",
      " \"that's\",\n",
      " 'the',\n",
      " 'their',\n",
      " 'them',\n",
      " 'then',\n",
      " 'there',\n",
      " 'they',\n",
      " 'think',\n",
      " 'this',\n",
      " 'thousands',\n",
      " 'tian',\n",
      " 'time',\n",
      " 'times',\n",
      " 'to',\n",
      " 'told',\n",
      " 'tool',\n",
      " 'truly',\n",
      " 'truth',\n",
      " 'try',\n",
      " 'university',\n",
      " 'up',\n",
      " 'use',\n",
      " 'used',\n",
      " 'users',\n",
      " 'using',\n",
      " 'violation',\n",
      " 'want',\n",
      " 'warned',\n",
      " 'was',\n",
      " 'we',\n",
      " \"we're\",\n",
      " 'week',\n",
      " 'were',\n",
      " 'wharton',\n",
      " 'what',\n",
      " 'when',\n",
      " 'where',\n",
      " 'will',\n",
      " 'with',\n",
      " 'without',\n",
      " 'world',\n",
      " 'would',\n",
      " 'write',\n",
      " 'writing',\n",
      " 'written',\n",
      " 'wrong',\n",
      " 'year',\n",
      " 'yet']\n"
     ]
    }
   ],
   "source": [
    "dtm.shape\n",
    "\n",
    "# check words in a paragraph\n",
    "p = 0 # paragraph id\n",
    "docs[p]\n",
    "[w for i,w in enumerate(words) if dtm[p][i]>0] \n",
    "\n",
    "pp.pprint(sorted(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['hello', \"it's\", 'a', 'helloooo', 'world', 'again', 'it', 'is']"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(doc):\n",
    "    \n",
    "    vocab = {}\n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    doc=doc.lower() # Convert the doc to lower case\n",
    "    tokens=doc.split() # Split the doc into a list of tokens by space\n",
    "    tokens=[token.strip(string.punctuation) for token in tokens if token.strip(string.punctuation)] \n",
    "    # Remove leading or trailing punctuations of each token and filter out empty strings\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            vocab[token]+=1\n",
    "        else:\n",
    "            vocab[token]=1 \n",
    "    return vocab\n",
    "def get_dtm(docs):\n",
    "    \n",
    "    # get all words\n",
    "    all_words = []\n",
    "    dtm = None\n",
    "    \n",
    "    # add your code here\n",
    "    tokenized_docs=[tokenize(doc) for doc in docs]\n",
    "    \n",
    "    for token in tokenized_docs:\n",
    "        for key in token.keys():\n",
    "            if key not in all_words:\n",
    "                all_words.append(key)\n",
    "            \n",
    "    dtm=np.zeros((len(docs), len(all_words)))\n",
    "    \n",
    "    for i in range(len(dtm)):\n",
    "        word=tokenized_docs[i]\n",
    "        for j in range(len(dtm[0])):\n",
    "            if all_words[j] in word.keys():\n",
    "                dtm[i][j]=word[all_words[j]]\n",
    "    \n",
    "    return dtm, all_words\n",
    "\n",
    "# test\n",
    "docs = [\"Hello , it's a helloooo world!\",\n",
    "       \"Again, it is hello world!\"]\n",
    "\n",
    "dtm, words = get_dtm(docs)\n",
    "dtm\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['hello', \"it's\", 'a', 'helloooo', 'world', 'again', 'it', 'is']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\"Hello , it's a helloooo world!\",\n",
    "       \"Again, it is hello world!\"]\n",
    "\n",
    "dtm, words = get_dtm(docs)\n",
    "dtm\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Analyze DTM Array (4 points)\n",
    "\n",
    "\n",
    "**Don't use any loop in this task**. You should use array operations to take the advantage of high performance computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named `analyze_dtm(dtm, words, docs)` as follows:\n",
    "- It takes an array `dtm`, an array of `words`, and an array of documents (denoted `docs`) as inputs, where `dtm` is the array you created from `docs` in Q3 with a shape of $(m \\times n)$, and `words` corresponds to the columns of `dtm`.\n",
    "- Calculate the document frequency for each word $j$, e.g., how many documents contain word $j$. Save the result to array $df$. $df$ has shape of $(n,)$ or $(1, n)$. \n",
    "- Normalize the word count per paragraph: divides word count, i.e., $dtm_{i,j}$, by the total number of words in document $i$. Save the result as an array named $tf$. $tf$ has shape of $(m,n)$. \n",
    "* For each $dtm_{i,j}$, calculate $tfidf_{i,j} = \\frac{tf_{i, j}}{1+log(df_j)}$, i.e., divide each normalized word count by the log of the document frequency of the word (add 1 to the denominator to avoid dividing by 0).  $tfidf$ has shape of $(m,n)$ \n",
    "* Print out the following:\n",
    "    \n",
    "    - the total number of words in the documents represented by `dtm` \n",
    "    - the number of documents and the number of unique words\n",
    "    - the most frequent top 10 words in this document    \n",
    "    - top-5 words that show in most of the documents, i.e. words with the top 5 largest $df$ values (print words first, then their values. ) \n",
    "    - the longest document in terms of the number of words. Print out this document.\n",
    "    - top-5 words with the largest $tfidf$ values in the longest document (show words and values) \n",
    "    - documents that contain `intelligence` word.\n",
    "\n",
    "Note, for all the steps, **do not use any loop**. Just use array functions and broadcasting for high performance computation.\n",
    "\n",
    "Your answer may be different from the example output, since words may have the same values in the dtm but are kept in different positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the documents: 705.0\n",
      "\n",
      "Number of documents: 26, Number of unique words: 314\n",
      "\n",
      "Most frequent top 10 words: ['the' 'to' 'and' 'a' 'that' 'he' 'it' 'in' 'of' 'is']\n",
      "values: [31. 25. 19. 17. 13. 12. 12. 12. 11. 10.] \n",
      "\n",
      "Top-5 words in most documents: ['the' 'and' 'to' 'a' 'in']\n",
      "values [20 15 14 12 11] \n",
      "\n",
      "The longest document in terms of the number of words: \"\"\"I think everybody is cheating ... I mean, it's happening. So what I'm asking students to do is just be honest with me,\"\" he said. \"\"Tell me what they use ChatGPT for, tell me what they used as prompts to get it to do what they want, and that's all I'm asking from them. We're in a world where this is happening, but now it's just going to be at an even grander scale.\"\"\"\n",
      " \n",
      "\n",
      "Top-5 words with the largest TF-IDF values in the longest document: ['what' 'me' \"i'm\" \"it's\" 'happening'] [0.05479452 0.04109589 0.02739726 0.02739726 0.02739726] \n",
      "\n",
      "Document IDs: [ 4 14]\n",
      "Documents containing 'intelligence': ['\"Some school districts have banned access to the bot, and not without reason. The artificial intelligence tool from the company OpenAI can compose poetry. It can write computer code. It can maybe even pass an MBA exam.\"\\n'\n",
      " '\"He readily admits he alternates between enthusiasm and anxiety about how artificial intelligence can change assessments in the classroom, but he believes educators need to move with the times.\"\\n']\n"
     ]
    }
   ],
   "source": [
    "def analyze_dtm(dtm, words, docs):\n",
    "\n",
    "    df=np.sum(dtm>0, axis=0) \n",
    "    \n",
    "    word_counts_per_doc = np.sum(dtm, axis=1, keepdims=True)\n",
    "    tf=dtm/word_counts_per_doc\n",
    "    \n",
    "    tfidf=tf/(1+np.log(df)[np.newaxis, :])\n",
    "    \n",
    "    #1.\n",
    "    total_words=np.sum(dtm)\n",
    "    print(f\"Total number of words in the documents: {total_words}\\n\")\n",
    "    \n",
    "    #2.\n",
    "    num_docs=dtm.shape[0]\n",
    "    num_unique_words=dtm.shape[1]\n",
    "    print(f\"Number of documents: {num_docs}, Number of unique words: {num_unique_words}\\n\")\n",
    "\n",
    "    \n",
    "    #3.\n",
    "    word_frequencies=np.sum(dtm, axis=0)\n",
    "    top_10_index=np.argsort(word_frequencies)[-10:][::-1]  \n",
    "    top_10_words=words[top_10_index]\n",
    "    top_10_values= word_frequencies[top_10_index]\n",
    "    print(\"Most frequent top 10 words:\", top_10_words)\n",
    "    print(\"values:\", top_10_values, \"\\n\")\n",
    "\n",
    "    \n",
    "    #4.\n",
    "    top_5_df_index=np.argsort(df)[-5:][::-1]\n",
    "    top_5_words_df=words[top_5_df_index]\n",
    "    top_5_df_values=df[top_5_df_index]\n",
    "    print(\"Top-5 words in most documents:\", top_5_words_df)\n",
    "    print(\"values\", top_5_df_values, \"\\n\")\n",
    "    \n",
    "    #5.\n",
    "    longest_doc_index=np.argmax(word_counts_per_doc)\n",
    "    longest_doc=docs[longest_doc_index]\n",
    "    print(\"The longest document in terms of the number of words:\", longest_doc, \"\\n\")\n",
    "    \n",
    "    #6.\n",
    "    top_5_tfidf_indices_longest_doc=np.argsort(tfidf[longest_doc_index])[-5:][::-1]\n",
    "    top_5_words_tfidf_longest_doc=words[top_5_tfidf_indices_longest_doc]\n",
    "    top_5_tfidf_values_longest_doc=tfidf[longest_doc_index, top_5_tfidf_indices_longest_doc]\n",
    "    print(\"Top-5 words with the largest TF-IDF values in the longest document:\", top_5_words_tfidf_longest_doc, top_5_tfidf_values_longest_doc, \"\\n\")\n",
    "    \n",
    "    #7.\n",
    "    intelligence_index=np.where(words == 'intelligence')[0]\n",
    "    docs_intelligence_index=np.where(dtm[:,intelligence_index]>0)[0]\n",
    "    if intelligence_index.size>0:\n",
    "        docs_with_intelligence=np.nonzero(dtm[:, intelligence_index])[0]\n",
    "        docs_containing_intelligence = docs[docs_with_intelligence]\n",
    "    else:\n",
    "        docs_containing_intelligence = np.array([])\n",
    "    print(\"Document IDs:\", docs_intelligence_index)\n",
    "    print(\"Documents containing 'intelligence':\", docs_containing_intelligence)\n",
    "    \n",
    "    \n",
    "    #test\n",
    "    words = np.array(words)\n",
    "docs = np.array(docs)\n",
    "\n",
    "analyze_dtm(dtm, words, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words:\n",
      "705.0\n",
      "\n",
      "the number of documents: 26, the number of unique words in the documents: 314\n",
      "\n",
      "The top 10 frequent words:\n",
      "['the' 'to' 'and' 'a' 'that' 'it' 'he' 'in' 'of' 'is'],\n",
      " values: [31. 25. 19. 17. 13. 12. 12. 12. 11. 10.]\n",
      "\n",
      "The top 5 words with highest df values:\n",
      "['the' 'and' 'to' 'a' 'in'],\n",
      "values: [20 15 14 12 11]\n",
      "\n",
      "The longest document:\n",
      " 24: \"\"\"I think everybody is cheating ... I mean, it's happening. So what I'm asking students to do is just be honest with me,\"\" he said. \"\"Tell me what they use ChatGPT for, tell me what they used as prompts to get it to do what they want, and that's all I'm asking from them. We're in a world where this is happening, but now it's just going to be at an even grander scale.\"\"\"\n",
      "\n",
      "\n",
      "The top 5 words with highest tf-idf values in the longest document:\n",
      "['what' 'me' 'happening' \"i'm\" 'tell'],\n",
      "values: [0.05479452 0.04109589 0.02739726 0.02739726 0.02739726]\n",
      "\n",
      "documents that contain word 'intelligence': \n",
      "Document IDs:[ 4 14],\n",
      "Text: ['\"Some school districts have banned access to the bot, and not without reason. The artificial intelligence tool from the company OpenAI can compose poetry. It can write computer code. It can maybe even pass an MBA exam.\"\\n'\n",
      " '\"He readily admits he alternates between enthusiasm and anxiety about how artificial intelligence can change assessments in the classroom, but he believes educators need to move with the times.\"\\n']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = np.array(words)\n",
    "docs = np.array(docs)\n",
    "\n",
    "analyze_dtm(dtm, words, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 (Bonus). Generating DTM by subword tokenization (2 points)\n",
    "\n",
    "Assume you only need to keep the top N most frequent words (e.g., N = 200) in the collection of documents. Redo Q3-Q4 as follows:\n",
    "\n",
    "- Use the subword tokenization you developed in Q2 to tokenize documents\n",
    "- Generate a dtm with only the top-N most frequent words in the entire collection.\n",
    "- Then analyze the dtm as in Q4.\n",
    "\n",
    "\n",
    "Describe and implement your ideas. Again, no loop should be used in your solution to Q4. **Don't just submit code. You need to explain your idea as markdowns. No score will be given if only code is submitted**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put everything together and test using main block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best practice to test your class\n",
    "# if your script is exported as a module,\n",
    "# the following part is ignored\n",
    "# this is equivalent to main() in Java\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    \n",
    "    print(\"\\n=======Q1: =========\\n\")\n",
    "    doc = \"Hello , it's a helloooo world!\"\n",
    "    vocab = tokenize(doc)\n",
    "    pp.pprint(vocab)\n",
    "    \n",
    "    print(\"\\n=======Q2: =========\\n\")\n",
    "    doc = \"Hello world, it's a helloooo world!\"\n",
    "\n",
    "    vocab_out, subwords = subword_tokenize(doc, num_merges = 9)\n",
    "\n",
    "    print(\"vocab:\")\n",
    "    pp.pprint(vocab_out)\n",
    "\n",
    "    print(\"subwords:\")\n",
    "    pp.pprint(subwords)\n",
    "    \n",
    "    print(\"\\n=======Q3: =========\\n\")\n",
    "    \n",
    "    docs = [\"Hello , it's a helloooo world!\",\n",
    "       \"Again, it is hello world!\"]\n",
    "\n",
    "    dtm, words = get_dtm(docs)\n",
    "    pp.pprint(dtm)\n",
    "    pp.pprint(words)\n",
    "    \n",
    "    print(\"\\n=======Q4: =========\\n\")\n",
    "    \n",
    "    docs = open(\"chatgpt.txt\", 'r').readlines()\n",
    "\n",
    "    dtm, words = get_dtm(docs)\n",
    "\n",
    "    words = np.array(words)\n",
    "    docs = np.array(docs)\n",
    "\n",
    "    analyze_dtm(dtm, words, docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
